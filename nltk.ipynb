{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and python is awsome.', 'The sky is pinkish-blue.', 'You shuld not eat cardboard']\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awsome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'shuld', 'not', 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example = 'Hello Mr. Smith, how are you doing today? The weather is great and python is awsome. The sky is pinkish-blue. You shuld not eat cardboard'\n",
    "\n",
    "print(sent_tokenize(example))\n",
    "print(word_tokenize(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = 'This is an example showing off stop word filtration.'\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence = [i for i in word_tokenize(sentence) if i not in STOP_WORDS]\n",
    "\n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'python', 'pytthon', 'python', 'pythonli']\n",
      "['it', 'is', 'veri', 'inport', 'to', 'be', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python', '.', 'all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['python', 'pythoner', 'pytthoning', 'pythoned', 'pythonly']\n",
    "\n",
    "print([ps.stem(w) for w in words])\n",
    "\n",
    "text = 'it is very inportant to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "\n",
    "print([ps.stem(w) for w in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "to = PunktSentenceTokenizer(text)\n",
    "\n",
    "sentences = to.tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    # chuckParser = nltk.RegexpParser(r'''Chuck: {<RB.?>*<VB.?>*<NNP>+<NN>?}\n",
    "    #                                     }<VB.?|IN|DT|TO>+{''')\n",
    "    # chuncks = chuckParser.parse(tags)\n",
    "    # chuncks.draw() \n",
    "\n",
    "    # names = nltk.ne_chunk(tags) # to igoner classes\n",
    "    # names.draw()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "better\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "print(lm.lemmatize('cat'))\n",
    "print(lm.lemmatize('better'))\n",
    "print(lm.lemmatize('better', pos='a'))\n",
    "print(lm.lemmatize('best', pos='a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THIAGO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\__init__.py\n",
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sample = gutenberg.raw('bible-kjv.txt')\n",
    "\n",
    "print(sent_tokenize(sample)[5:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['plan', 'program', 'programme']\n",
      "plan.n.01\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print(len(syns))\n",
    "\n",
    "print([lemma.name() for lemma in syns[0].lemmas()])\n",
    "\n",
    "print(syns[0].name())\n",
    "\n",
    "print(syns[0].definition())\n",
    "\n",
    "print(syns[0].examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upright', 'soundly', 'proficient', 'skillful', 'good', 'goodness', 'skilful', 'commodity', 'serious', 'estimable', 'unspoilt', 'unspoiled', 'beneficial', 'safe', 'in_force', 'well', 'practiced', 'expert', 'thoroughly', 'honorable', 'sound', 'respectable', 'full', 'dear', 'near', 'dependable', 'ripe', 'honest', 'secure', 'just', 'salutary', 'undecomposed', 'adept', 'trade_good', 'right', 'effective', 'in_effect'}\n",
      "{'ill', 'bad', 'evil', 'evilness', 'badness'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('good'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "\n",
    "print(w1.wup_similarity(w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle\n",
    "\n",
    "documents = [(list(movie_reviews.words(field)), category)\n",
    "                for category in movie_reviews.categories()\n",
    "                for field in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "\n",
    "all_words = nltk.FreqDist([w.lower() for w in movie_reviews.words()])\n",
    "\n",
    "print(all_words.most_common(15))\n",
    "\n",
    "print(all_words['stupid'])\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    features = []\n",
    "    words = set(document)\n",
    "    for w in word_features:\n",
    "        features.append(1 if w in words else 0)\n",
    "\n",
    "    return features\n",
    "\n",
    "# print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "\n",
    "training_data = [(find_features(rev), 1 if category == 'pos' else 0) for (rev, category) in documents]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 [==============================] - 2s 15ms/step - loss: 0.5990 - accuracy: 0.6686 - val_loss: 0.4802 - val_accuracy: 0.7650\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.2592 - accuracy: 0.8907 - val_loss: 0.3953 - val_accuracy: 0.8267\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0623 - accuracy: 0.9829 - val_loss: 0.5432 - val_accuracy: 0.8267\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 0.8350\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6957 - val_accuracy: 0.8350\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 7.7069e-04 - accuracy: 1.0000 - val_loss: 0.7283 - val_accuracy: 0.8350\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 4.7718e-04 - accuracy: 1.0000 - val_loss: 0.7559 - val_accuracy: 0.8350\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 3.3480e-04 - accuracy: 1.0000 - val_loss: 0.7776 - val_accuracy: 0.8350\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 2.5182e-04 - accuracy: 1.0000 - val_loss: 0.7964 - val_accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 1.9326e-04 - accuracy: 1.0000 - val_loss: 0.8150 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251f83ef820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "NAME = f'sentiment-500x3{ int(time.time()) }'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f\"logs/{NAME}\")\n",
    "\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X =  np.array(pickle.load(pickle_in))\n",
    "\n",
    "pickle_in = open(\"y.pickle\", \"rb\")\n",
    "y =  np.array(pickle.load(pickle_in))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size = 32,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.3,\n",
    "    callbacks = [tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('review.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('review.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('positive.txt') as positive_file:\n",
    "    positive = positive_file.read()\n",
    "\n",
    "with open('negative.txt') as negative_file:\n",
    "    negative = negative_file.read()\n",
    "\n",
    "documents = []\n",
    "for review in positive.split('\\n'):\n",
    "    documents.append((review, 'pos'))\n",
    "\n",
    "for review in negative.split('\\n'):\n",
    "    documents.append((review, 'neg'))\n",
    "\n",
    "all_words = []\n",
    "\n",
    "positive_words = word_tokenize(positive)\n",
    "negative_words = word_tokenize(negative)\n",
    "\n",
    "for word in positive_words:\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "\n",
    "for word in negative_words:\n",
    "    all_words.append(word.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    features = []\n",
    "    words = set(document)\n",
    "    for w in word_features:\n",
    "        features.append(1 if w in words else 0)\n",
    "\n",
    "    return features\n",
    "\n",
    "# print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "\n",
    "training_data = [(find_features(rev), 1 if category == 'pos' else 0) for (rev, category) in tqdm(documents)]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "pickle_out = open(\"Xsmall.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"ysmall.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234/234 [==============================] - 4s 15ms/step - loss: 0.6069 - accuracy: 0.7142 - val_loss: 1.4015 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "234/234 [==============================] - 3s 14ms/step - loss: 0.5990 - accuracy: 0.7141 - val_loss: 1.3121 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "234/234 [==============================] - 3s 13ms/step - loss: 0.5968 - accuracy: 0.7142 - val_loss: 1.3125 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.5941 - accuracy: 0.7144 - val_loss: 1.4035 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "234/234 [==============================] - 2s 8ms/step - loss: 0.5941 - accuracy: 0.7161 - val_loss: 1.0948 - val_accuracy: 0.1303\n",
      "Epoch 6/10\n",
      "234/234 [==============================] - 1s 6ms/step - loss: 0.5945 - accuracy: 0.7161 - val_loss: 1.0606 - val_accuracy: 0.0500\n",
      "Epoch 7/10\n",
      "234/234 [==============================] - 1s 6ms/step - loss: 0.5929 - accuracy: 0.7168 - val_loss: 1.1923 - val_accuracy: 0.0475\n",
      "Epoch 8/10\n",
      "234/234 [==============================] - 2s 6ms/step - loss: 0.5899 - accuracy: 0.7185 - val_loss: 1.3340 - val_accuracy: 0.0228\n",
      "Epoch 9/10\n",
      "234/234 [==============================] - 2s 7ms/step - loss: 0.5877 - accuracy: 0.7189 - val_loss: 1.1483 - val_accuracy: 0.0241\n",
      "Epoch 10/10\n",
      " 29/234 [==>...........................] - ETA: 1s - loss: 0.5853 - accuracy: 0.7220"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "NAME = f'sentiment-500x3{ int(time.time()) }'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f\"logs/{NAME}\")\n",
    "\n",
    "pickle_in = open(\"Xsmall.pickle\", \"rb\")\n",
    "X =  np.array(pickle.load(pickle_in))\n",
    "\n",
    "pickle_in = open(\"ysmall.pickle\", \"rb\")\n",
    "y =  np.array(pickle.load(pickle_in))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size = 32,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.3,\n",
    "    callbacks = [tensorboard]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0d9a7cd1f0e24e1e06e6e11ca7ef582e03455e211f245ad365318db042b07ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
